#!/usr/bin/env python
# coding: utf-8

import numpy as np
from sklearn.gaussian_process import GaussianProcessRegressor
from matplotlib import pyplot as plt
import itertools
from sklearn.gaussian_process.kernels import Matern
import scipy
import sys
import os
import pyDOE
import copy

sys.path.append(os.path.abspath("/home/victor/These/Bayesian SWE/bayesian_optimization_VT"))
import acquisition_function as acq
import bo_plot as bplt


# -----------------------------------------------------------------------------
def EGO_brute(gp_, true_function, X_, niterations=10, plot=True):
    """
    EGO performed with brute force: Brute search on vector X_

    Args:
        gp_ (GaussianProcessRegressor): GP of modelling the function to minimize
        X_ ([npoints,nfeatures] array): vector of points on which to compute and
                                        search the maximum of EI
        true_function (func): function to minimize
        niterations (int): number of iterations to perform
        plot (bool): Plot the successive iteration (only for 1D and 2D problems)

    Output:
        GaussianProcessRegressor: GP of the function after the niterations
    """

    print 'Resolution of the brute search: ' + str(X_[1] - X_[0])

    if plot and X_.ndim > 2:
        print 'No plot as dim of input > 2'
        plot = False

    gp = copy.copy(gp_)
    i = 0
    while i < niterations:
        print 'Iteration ' + str(i + 1) + ' of ' + str(niterations)

        EI_computed = acq.gp_EI_computation(gp, X_)
        next_to_evaluate = acq.acquisition_maxEI_brute(gp, X_)
        value_evaluated = true_function(next_to_evaluate)
        if plot:
            if X_.ndim == 1:
                bplt.plot_1d_strategy(gp, X_, true_function, nsamples = 5,
                                      criterion = EI_computed,
                                      next_to_evaluate = next_to_evaluate)
            elif X_.ndim == 2:
                bplt.plot_2d_strategy(gp, X_, true_function, criterion = EI_computed,
                                      next_to_evaluate = next_to_evaluate)

        X = np.vstack([gp.X_train_, next_to_evaluate])
        # X = X[:,np.newaxis]
        y = np.append(gp.y_train_, value_evaluated)
        gp.fit(X, y)
        i += 1
        print 'Best value found so far ' + str(gp.X_train_[gp.y_train_.argmin()])

    print '  Best value found after ' + str(niterations) + ' iterations: ' \
        + str(gp.X_train_[gp.y_train_.argmin()])
    return gp


# -----------------------------------------------------------------------------
def EGO_analytical(gp_, true_function, X_ = None, niterations = 10,
                   plot = False, nrestart = 20, bounds = None):
    """
    EGO performed with optimization on the EI

    Args:
        gp_ (GaussianProcessRegressor): GP of modelling the function to minimize
        true_function (func): function to minimize
        niterations (int): number of iterations to perform
        plot (bool): Plot the successive iteration (only for 1D and 2D problems)
        X_ ([npoints,nfeatures] array): vector of points for the plots


    Output:
        GaussianProcessRegressor: GP of the function after the niterations
    """
    # if plot and X_.ndim>2:
    #     print 'No plot as dim of input > 2'
    #     plot = False

    gp = copy.copy(gp_)
    i = 0
    while i < niterations:
        print 'Iteration ' + str(i + 1) + ' of ' + str(niterations)
        if X_ is not None:
            EI_computed = acq.gp_EI_computation(gp, X_)

        next_to_evaluate = acq.acquisition_maxEI_analytical_gradientfree(gp, nrestart, bounds)
        value_evaluated = true_function(next_to_evaluate)

        if plot:
            if X_.ndim == 1:
                bplt.plot_1d_strategy(gp, X_, true_function, nsamples = 5,
                                      criterion = EI_computed,
                                      next_to_evaluate = next_to_evaluate)

            elif X_.ndim == 2:
                bplt.plot_2d_strategy(gp, X_, true_function,
                                      criterion = EI_computed,
                                      next_to_evaluate = next_to_evaluate)

        X = np.vstack([gp.X_train_, next_to_evaluate])
        # X = X[:,np.newaxis]
        y = np.append(gp.y_train_, value_evaluated)
        gp.fit(X, y)
        i += 1
        print '  Best value yet ' + str(gp.X_train_[gp.y_train_.argmin()])

    print '---  Best value found after ' + str(niterations) + ' iterations: ' \
        + str(gp.X_train_[gp.y_train_.argmin()])
    return gp


# --------------------------------------------------------------------------
def qEI_brute(gp_, true_function, X_=np.linspace(0, 5, 100), q=3,
              niterations=10, nsim=1000):
    """
    q steps EI performed with brute force: Brute search on vector X_
    """
    gp = copy.copy(gp_)
    i = 0
    nn = X_.shape[0]
    rshape = q * [nn]
    qEI_to_evaluate = np.asarray([np.vstack(np.array(comb))
                                  for comb in itertools.product(X_, repeat=q)]).squeeze()

    while i < niterations:
        bplt.plot_gp(gp, X_, true_function=true_function, nsamples=5, show=False)
        qEI_computed = acq.gp_qEI_computation_brute(gp, qEI_to_evaluate, nsim).reshape(rshape)
        next_to_evaluate = X_[np.asarray(np.unravel_index(qEI_computed.argmax(),
                                                          qEI_computed.shape))]
        value_evaluated = true_function(next_to_evaluate)
        [plt.axvline(nextpoint, ls = '--', color = 'red')
         for nextpoint in next_to_evaluate]

        X = np.append(gp.X_train_, next_to_evaluate)
        X = X[:, np.newaxis]
        y = np.append(gp.y_train_, value_evaluated)
        gp.fit(X, y)
        print i
        i += 1
        plt.show()
    return gp


# -----------------------------------------------------------------------------
def IAGO_brute(gp_, true_function, candidates, X_, niterations=10, M=10,
               nsamples = 1000, plot = True):
    """
    Informational Approach to Global Optimization method applied

    Args:
        gp (GaussianProcessRegressor): GP of modelling the function to minimize
        true_function (func): function to minimize
        candidates ([npoints,nfeatures] array): vector of candidates
        X_ ([npoints,nfeatures] array): vector of points for the plots
        niterations (int): number of iterations to perform
        M (int): number of quantiles to compute for the estimation of entropy
        nsamples (int): number of MC samples
                        to estimate distribution of minimizer
        plot (bool): Plot the successive iteration (only for 1D and 2D problems)

    Output:
        GaussianProcessRegressor: GP of the function after the niterations
    """
    gp = copy.copy(gp_)
    i = 0
    while i < niterations:
        print 'Iteration ' + str(i + 1) + ' of ' + str(niterations)

        cond_entropy = acq.conditional_entropy(gp, candidates, X_, M, nsamples)
        next_to_evaluate = candidates[cond_entropy.argmin()]
        value_evaluated = true_function(next_to_evaluate)

        if plot:
            if X_.ndim == 1:
                bplt.plot_1d_strategy(gp, candidates, true_function, nsamples = 5,
                                      criterion=-cond_entropy,
                                      next_to_evaluate=next_to_evaluate)
            elif X_.ndim == 2:
                bplt.plot_2d_strategy(gp, X_, true_function,
                                      criterion=cond_entropy,
                                      next_to_evaluate=next_to_evaluate)

        X = np.vstack([gp.X_train_, next_to_evaluate])
        # X = X[:,np.newaxis]
        y = np.append(gp.y_train_, value_evaluated)
        gp.fit(X, y)
        i += 1
        print 'Best value found so far ' +\
            str(gp.X_train_[gp.y_train_.argmin()])

    print '  Best value found after ' + str(niterations) + ' iterations: ' \
        + str(gp.X_train_[gp.y_train_.argmin()])
    return gp


# --------------------------------------------------------------------------
def exploEGO(gp_, true_function, idx_U, X_ = None, niterations = 10,
             plot = False, nrestart = 20, bounds = None):
    """
    exploEGO performed with optimization on the EI

    Args:
        gp_ (GaussianProcessRegressor): GP of modelling the function
                                        to minimize
        true_function (func): function to minimize
        niterations (int): number of iterations to perform
        plot (bool): Plot the successive iteration
                     (only for 1D and 2D problems)
        X_ ([npoints,nfeatures] array): vector of points for the plots


    Output:
        GaussianProcessRegressor: GP of the function after the niterations
    """
    # if plot and X_.ndim>2:
    #     print 'No plot as dim of input > 2'
    #     plot = False

    gp = copy.copy(gp_)
    i = 0
    if idx_U is None:
        print 'No index for exploration, classical EGO performed'
    while i < niterations:
        print 'Iteration ' + str(i + 1) + ' of ' + str(niterations)

        EI_computed = acq.gp_EI_computation(gp, X_)
        next_to_evaluate, distance = acq.acquisition_exploEI_analytical(gp, nrestart, bounds, idx_U)
        value_evaluated = true_function(next_to_evaluate)

        if plot:
            if X_.ndim == 1:
                bplt.plot_1d_strategy(gp, X_, true_function, nsamples=5,
                                      criterion=EI_computed,
                                      next_to_evaluate=next_to_evaluate)

            elif X_.ndim == 2:
                bplt.plot_2d_strategy(gp, X_, true_function,
                                      criterion=EI_computed,
                                      next_to_evaluate=next_to_evaluate)

        X = np.vstack([gp.X_train_, next_to_evaluate])
        # X = X[:,np.newaxis]
        y = np.append(gp.y_train_, value_evaluated)
        gp.fit(X, y)
        i += 1
        print '  Best value yet ' + str(gp.X_train_[gp.y_train_.argmin()])
        print '  Maximum distance: ' + str(distance)

    print '---  Best value found after ' + str(niterations) + ' iterations: ' \
        + str(gp.X_train_[gp.y_train_.argmin()])
    return gp


def eval_array_separate_variables(gp, value_to_change, value_to_fix, idx_to_fix):
    _, ndim = gp.X_train_.shape
    npoints, nchange = value_to_change.shape
    idx_to_change = filter(lambda i: i in range(ndim) and i not in idx_to_fix, range(ndim))

    if len(idx_to_change) + len(idx_to_fix) != ndim:
        print 'Dimensions do not match!'
    # rep_fixed = np.tile(value_to_fix, npoints).reshape(npoints, len(idx_to_fix))
    eval_array = np.zeros([npoints, ndim])
    eval_array[:, idx_to_change] = value_to_change
    eval_array[:, idx_to_fix] = value_to_fix
    return eval_array


def slicer_gp_predict(gp, value_to_fix, idx_to_fix, return_std=True):
    def fun_to_return(value_to_change):
        evalsep = eval_array_separate_variables(gp, value_to_change, value_to_fix, idx_to_fix)
        return gp.predict(evalsep, return_std=return_std)
    return fun_to_return


def find_minimum_sliced(gp, value_to_fix, idx_to_fix, bounds = None, nrestart = 10):
    fun_ = slicer_gp_predict(gp, value_to_fix, idx_to_fix, return_std=False)
    fun = lambda X_: fun_(np.atleast_2d(X_).T)
    optim_number = 1
    rng = np.random.RandomState()
    dim = gp.X_train_.shape[1] - len(idx_to_fix)
    if bounds is None:
        bounds = dim * [(0, 1)]
    x0 = [rng.uniform(bds[0], bds[1], 1) for bds in bounds]
    if len(x0) == 1:
        [x0] = x0
    current_minimum = scipy.optimize.minimize(fun, x0=x0, bounds=bounds)
    while optim_number <= nrestart:
        x0 = [rng.uniform(bds[0], bds[1], 1) for bds in bounds]
        if len(x0) == 1:
            [x0] = x0

        optim = scipy.optimize.minimize(fun, x0=x0, bounds = bounds)
        if optim.fun < current_minimum.fun:
            current_minimum = optim
        optim_number += 1
    return current_minimum


def PI_alpha_fix(gp, alpha, value_to_fix, idx_to_fix,
                 X_, bounds = None, nrestart = 10):
    minimum_fix = find_minimum_sliced(gp, value_to_fix, idx_to_fix, bounds, nrestart)
    # minimizer = minimum_fix.x
    minimum = minimum_fix.fun
    if X_.ndim == 1:
        X_ = np.atleast_2d(X_).T
    sliced_fun = slicer_gp_predict(gp, value_to_fix, idx_to_fix, return_std = True)
    y_mean, y_std = sliced_fun(X_)
    m = alpha * minimum - y_mean
    s = y_std
    return acq.probability_of_improvement(m, s)


def PI_alpha_allspace(gp, alpha, idx_to_explore, X_minimize,
                      X_explore, bounds = None, nrestart = 10):
    total_grid = np.empty([len(X_explore), len(X_minimize)])
    for index, x_fix in enumerate(X_explore):
        total_grid[index, :] = PI_alpha_fix(gp, alpha, x_fix, idx_to_explore,
                                            X_minimize, bounds, nrestart)
    return total_grid.mean(0)


def PI_alpha_check_tol(gp, idx_to_explore, X_search, Xfix_grid, ptol = 1.0,
                       bounds = None, nrestart = 10, delta_alpha = 0.01):
    if ptol > 1.0:
        print 'No solution for ptol > 1, set at 1.0'
        ptol = 1.0
    alpha = 1.0
    prob_alpha_min = PI_alpha_allspace(gp, alpha, idx_to_explore, X_search, Xfix_grid,
                                       bounds = bounds, nrestart = nrestart)
    while np.all(prob_alpha_min < ptol):
        alpha += delta_alpha
        prob_alpha_min = PI_alpha_allspace(gp, alpha, idx_to_explore, X_search, Xfix_grid,
                                           bounds = bounds, nrestart = nrestart)
    return alpha, Xfix_grid[prob_alpha_min.argmax()]


if __name__ == '__main__':
    rng = np.random.RandomState()
    X = rng.uniform(0, 1, 5) + [0.0, 1.0, 2.0, 3.0, 4.0]
    X = np.atleast_2d(X).T
    X = pyDOE.lhs(n=1, samples = 5)
    expl_sin = lambda X: np.exp(-np.ravel(X) / 5) * np.sin(np.ravel(X) * 2)
    higdon2002 = lambda X: 0.2 * np.sin(2 * np.pi * np.ravel(X) * 4) \
        + np.sin(2 * np.pi * np.ravel(X))

    # Evaluation
    true_function = higdon2002
    y = true_function(X)

    # Initialization GP
    gp = GaussianProcessRegressor(kernel = Matern(0.1))
    X_ = np.linspace(0, 1, 1000)
    # Fit
    gp.fit(X, y)

    fig = plt.figure()
    ax1 = fig.add_subplot(211)
    bplt.plot_gp(gp, X_, true_function = true_function, nsamples = 5, show=False)
    ax2 = fig.add_subplot(212)
    plt.plot(X_, acq.gp_EI_computation(gp, X_.T), label = 'EI')
    entropy_test = acq.conditional_entropy(gp, np.linspace(0, 1, 100), X_, M=10, nsamples = 1000)
    ax2b = ax2.twinx()
    plt.plot(np.linspace(0, 1, 100), -entropy_test, 'r--', label = 'condEntropy')
    plt.legend()
    plt.show()

    # EGO ------------------------------------
    gp_EGO = EGO_brute(gp_ = gp, X_ = X_, true_function = true_function,
                       niterations = 1, plot = True)
    gp.fit(X, y)  # Reset

    # EGO analytical -------------------------
    gp_analytical = EGO_analytical(gp, true_function, niterations = 20, X_= X_,
                                   bounds = [(0, 5)], plot = False)
    gp.fit(X, y)  # Reset

    # qEGO ------------------------------------
    gp_qEI = qEI_brute(gp_ = gp, true_function = true_function, q=2,
                       niterations = 5, nsim = 1000)
    gp.fit(X, y)  # Reset

    # IAGO -----------------------------------
    gp_IAGO = IAGO_brute(gp, true_function, np.linspace(0, 1, 100), X_,
                         niterations = 5, M = 5, nsamples = 1000, plot = True)
    gp.fit(X, y)  # Reset

    # exploEGO -------------------------------
    gp_explo_EGO = exploEGO(gp, true_function, 0, X_,
                            bounds = np.array([[0, 1]]))

    # *. 2D test ---------------------------------------------------------------
    def quadratic(X):
        X = np.atleast_2d(X)
        # return (X[:,0] - 2.5)**2 + (X[:,1] - 1)**2
        return 1 + (X[:, 1] - 0.3)**2 + (X[:, 0] - 0.9)**2  # + (X[:,2] - 3)**2

    def branin_2d(X):
        """ Scaled branin function:
        global minimizers are
        [0.124, 0.818], [0.54277, 0.1513], [0.96133, 0.16466]
        """
        X = np.atleast_2d(X)
        x1 = X[:, 0] * 15.0 - 5
        x2 = X[:, 1] * 15.0
        return 10 + 10 * (1 - (1 / (8 * np.pi))) * np.cos(x1) +\
            (x2 - (5.1 / (4 * np.pi**2)) * x1**2 + (5 / np.pi) * x1 - 6)**2
    # Xmin1 = [0.124, 0.818]
    # Xmin2 = [0.51277, 0.1513]
    # Xmin3 = [0.96133, 0.16466]

    def two_valleys(X, sigma = 1, rotation_angle = 0):
        X = np.atleast_2d(X) * 2 - 1

        X[:, 0] = np.cos(rotation_angle) * X[:, 0] - np.sin(rotation_angle) * X[:, 1]
        X[:, 1] = np.sin(rotation_angle) * X[:, 0] + np.cos(rotation_angle) * X[:, 1]
        X = (X + 1) / 2
        k = X[:, 0] * 6 - 3
        u = X[:, 1]
        return -u * np.exp(-(k - 1)**2 / sigma**2) \
            - (1 - u) * 1.01 * np.exp(-(k + 1)**2 / sigma**2) \
            + np.exp(-k**2 / sigma**2) + 1 / (sigma**2)

    def gaussian_peaks(X):
        X = np.atleast_2d(X)
        x, y = X[:, 0] * 5, X[:, 1] * 5
        return 0.8 * np.exp(-(((x)**2 + (y)**2) / 3)) \
            + 1.2 * np.exp(-(((y - 2.5)**2) + (x - 2.0)**2) / 1) \
            + np.exp(-(x - 0.5)**2 / 3 - (y - 4)**2 / 2) \
            + 0.8 * np.exp(-(x - 5)**2 / 4 - (y)**2 / 4) \
            + np.exp(-(x - 5)**2 / 4 - (y - 5)**2 / 4) \
            + (1 / (1 + x + y)) / 25  # + 50 * np.exp((-(y - 2.5)**2 + -(x - 5)**2) / 2)
    function_2d = lambda X: two_valleys(X, 1, np.pi / 4)
    # function_2d = rosenbrock_general
    rng = np.random.RandomState()
    ndim = 2

    # initial_design_2d = np.array([[1,1],[2,2],[3,3],[4,4], [5,2], [1,4],[0,0],[5,5], [4,1]])/5.0
    initial_design_2d = pyDOE.lhs(n=2, samples=20, criterion='maximin',
                                  iterations=50)
    response_2d = function_2d(initial_design_2d)
    gp = GaussianProcessRegressor(kernel = Matern(np.ones(ndim) / 5.0))
    gp.fit(initial_design_2d, response_2d)

    X_ = np.linspace(0, 1, 200)
    xx, yy = np.meshgrid(X_, X_, indexing = 'ij')
    all_combinations = np.array([xx, yy]).T.reshape(-1, 2, order = 'F')
    EI_criterion = acq.gp_EI_computation(gp, all_combinations)
    # cond_entropy_2d = acq.conditional_entropy(gp, all_combinations,
    #                                           all_combinations, M=5,
    #                                           nsamples=100)

    bplt.plot_2d_strategy(gp, all_combinations, function_2d, EI_criterion)
    # bplt.plot_2d_strategy(gp, all_combinations, function_2d, -cond_entropy_2d)


    # EGO brute ------------------------------
    gp_brute = EGO_brute(gp, function_2d, all_combinations, niterations=5, plot=True)
    EI_criterion_brute = acq.gp_EI_computation(gp_brute, all_combinations)
    bplt.plot_2d_strategy(gp_brute, all_combinations, function_2d, EI_criterion_brute)


    # EGO analytical -------------------------
    gp_analytical = EGO_analytical(gp, function_2d, X_ = all_combinations, niterations = 50,
                                   plot = False, nrestart = 30, bounds = [(0, 1)] * 2)
    EI_criterion_analytical = acq.gp_EI_computation(gp_analytical, all_combinations)
    bplt.plot_2d_strategy(gp_analytical, all_combinations, function_2d, EI_criterion_analytical)


    # Explo EGO ------------------------------
    gp_explo_EGO = exploEGO(gp, function_2d, idx_U = [1], X_= all_combinations,
                            niterations = 50, plot = False, nrestart = 50,
                            bounds = np.array([[0, 1], [0, 1]]))
    EI_criterion_analytical = acq.gp_EI_computation(gp_explo_EGO, all_combinations)
    bplt.plot_2d_strategy(gp_explo_EGO, all_combinations, function_2d, EI_criterion_analytical)


    # IAGO -----------------------------------
    X_reduced = np.linspace(0, 1, 10)
    xxr, yyr = np.meshgrid(X_, X_, indexing = 'ij')
    all_combinations_reduced = np.array([xxr, yyr]).T.reshape(-1, 2, order = 'F')
    gp_IAGO = IAGO_brute(gp, function_2d,
                         candidates=all_combinations_reduced,
                         X_=all_combinations_reduced,
                         niterations = 5, M = 2, nsamples = 100, plot = True)




    # Slicer Test -----------------------------------------------------
    y_2d_pred, y_2d_std = np.asarray(gp.predict(all_combinations, return_std = True))
    plt.contourf(xx, yy, y_2d_pred.reshape(200, 200))
    plt.colorbar()
    plt.subplot(1, 2, 2)
    gppred = slicer_gp_predict(gp, [0.5], [0])
    idx = 1
    y_mean05, y_std05 = gppred(np.atleast_2d(X_).T)

    alpha_1, k_check_1 = PI_alpha_check_tol(gp, [idx], X_, X_, delta_alpha = 0.05, ptol = 1.0)
    alpha_99, k_check_99 = PI_alpha_check_tol(gp, [idx], X_, X_, delta_alpha = 0.05, ptol = 0.99)
    alpha_95, k_check_95 = PI_alpha_check_tol(gp, [idx], X_, X_, delta_alpha = 0.05, ptol = 0.95)
    alpha_90, k_check_90 = PI_alpha_check_tol(gp, [idx], X_, X_, delta_alpha = 0.05, ptol = 0.9)
    print alpha_1, k_check_1
    print alpha_99, k_check_99
    print alpha_95, k_check_95
    print alpha_90, k_check_90

    plt.subplot(1, 2, 1)
    plt.contourf(xx, yy, y_2d_pred.reshape(200, 200))
    plt.axvline(k_check_1, ls = '--')
    plt.axvline(k_check_99, ls = '--')
    plt.axvline(k_check_95, ls = '--')
    plt.axvline(k_check_90, ls = '--')
    plt.colorbar()
    plt.subplot(1, 2, 2)
    plt.plot(X_, PI_alpha_allspace(gp, 1., [idx], X_, X_))
    plt.plot(X_, PI_alpha_allspace(gp, 1.5, [idx], X_, X_))
    plt.plot(X_, PI_alpha_allspace(gp, 1.8, [idx], X_, X_))
    plt.plot(X_, PI_alpha_allspace(gp, alpha_1, [idx], X_, X_))
    plt.plot(X_, PI_alpha_allspace(gp, alpha_99, [idx], X_, X_))
    plt.plot(X_, PI_alpha_allspace(gp, alpha_95, [idx], X_, X_))
    plt.plot(X_, PI_alpha_allspace(gp, alpha_90, [idx], X_, X_))
    plt.axhline(1.00, ls ='--', color = 'black')
    plt.axhline(0.99, ls ='--', color = 'black', alpha = 0.8)
    plt.axhline(0.95, ls ='--', color = 'black', alpha = 0.6)
    plt.axhline(0.90, ls ='--', color = 'black', alpha = 0.4)
    plt.title('idx of U: ' + str(idx))
    plt.show()

    gppred = slicer_gp_predict(gp, [k_check_1], [0])
    y_meancheck, y_stdcheck = gppred(np.atleast_2d(X_).T)
    bplt.plot_mean_std(X_, y_meancheck, y_stdcheck, show = False, label = 'check')

    gppred = slicer_gp_predict(gp, [k_check_95], [0])
    y_meancheck, y_stdcheck = gppred(np.atleast_2d(X_).T)
    bplt.plot_mean_std(X_, y_meancheck, y_stdcheck, show = False, label = 'check95', color = 'g')

    gppred = slicer_gp_predict(gp, [0.5], [0])
    y_meancheck, y_stdcheck = gppred(np.atleast_2d(X_).T)
    bplt.plot_mean_std(X_, y_meancheck, y_stdcheck, show = False, label = '0.5', color='b')

    gppred = slicer_gp_predict(gp, [0.2], [0])
    y_meancheck, y_stdcheck = gppred(np.atleast_2d(X_).T)
    bplt.plot_mean_std(X_, y_meancheck, y_stdcheck, show = False, label = '0.7', color = 'm')
    plt.legend()
    plt.xlabel('u')
    plt.show()





    # Ndimensional Test ---------------------------------------------------------
    def rosenbrock_general(X):
        X = np.atleast_2d(X)
        X = X * 15 - 5
        return np.sum(100.0 * (X[:, 1:] - X[:, :-1]**2.0)**2.0 + (1 - X[:, :-1])**2.0, 1)

    NDIM = 10

    initial_design_4d = pyDOE.lhs(n=NDIM, samples=10 * NDIM,
                                  criterion='maximin', iterations=50)
    response_4d = rosenbrock_general(initial_design_4d)
    gp4d = GaussianProcessRegressor(kernel = Matern(np.ones(NDIM) / 5))
    gp4d.fit(initial_design_4d, response_4d)

    gp_analytical_4d = EGO_analytical(gp4d, rosenbrock_general,
                                      X_ = initial_design_4d, niterations = 100,
                                      plot = False, nrestart=50,
                                      bounds = [(0, 1)] * NDIM)
